{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNa2vshGJWh7j2xcm+2JCrk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c993b20a6b584599b1743f987704fd63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_569f8300629a4455b8aa054c8b6f8ace",
              "IPY_MODEL_7a8f4100d68c458b8cf9d1cefbef25dc",
              "IPY_MODEL_d725a2afc2ec47799594d3a360552b8f"
            ],
            "layout": "IPY_MODEL_d4c4d914658b43da8d6a324c3db4a7dc"
          }
        },
        "569f8300629a4455b8aa054c8b6f8ace": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_699c78c1c8b54a4bb831d70ba0d24d8f",
            "placeholder": "​",
            "style": "IPY_MODEL_3d8aae51801f42b7ac622bdb8836a857",
            "value": "model.pt: 100%"
          }
        },
        "7a8f4100d68c458b8cf9d1cefbef25dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b982d23d74740ea9fff91e4fd633944",
            "max": 356235922,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0537f75172a5472a9ba1cd3c487bf24b",
            "value": 356235922
          }
        },
        "d725a2afc2ec47799594d3a360552b8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2b6619a6a9c43cc94f914c1a40f75a7",
            "placeholder": "​",
            "style": "IPY_MODEL_0631bf2b5ce243329c8e11567442f23f",
            "value": " 356M/356M [00:07&lt;00:00, 45.8MB/s]"
          }
        },
        "d4c4d914658b43da8d6a324c3db4a7dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "699c78c1c8b54a4bb831d70ba0d24d8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d8aae51801f42b7ac622bdb8836a857": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4b982d23d74740ea9fff91e4fd633944": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0537f75172a5472a9ba1cd3c487bf24b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f2b6619a6a9c43cc94f914c1a40f75a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0631bf2b5ce243329c8e11567442f23f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/strumberr/fine-tuned-carlsen-model/blob/main/Current_Best_Chess_Magnus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59L12kTn_fE7",
        "outputId": "f9da6add-44bf-4083-8631-2a263e4a1949"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Collecting python-chess\n",
            "  Downloading python_chess-1.999-py3-none-any.whl.metadata (776 bytes)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Collecting chess<2,>=1 (from python-chess)\n",
            "  Downloading chess-1.11.2.tar.gz (6.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
            "Downloading python_chess-1.999-py3-none-any.whl (1.4 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m117.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m91.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m93.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: chess\n",
            "  Building wheel for chess (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for chess: filename=chess-1.11.2-py3-none-any.whl size=147775 sha256=b730c7dbf9850b629c219d1d055bfc3a3fdde5842056aa7d80f448e66d77f045\n",
            "  Stored in directory: /root/.cache/pip/wheels/fb/5d/5c/59a62d8a695285e59ec9c1f66add6f8a9ac4152499a2be0113\n",
            "Successfully built chess\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, chess, python-chess, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed chess-1.11.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 python-chess-1.999\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers python-chess torch numpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "import chess.pgn\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import GPT2LMHeadModel, AutoTokenizer, Trainer, TrainingArguments, GPT2Model, AutoModel\n",
        "from torch.utils.data import Dataset\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# # Authenticate with Hugging Face\n",
        "# hf_token = userdata.get('HF_TOKEN')\n",
        "# if hf_token:\n",
        "#     login(token=hf_token)\n",
        "# else:\n",
        "#     raise ValueError(\"HF_TOKEN not found in Colab secrets. Please add it at https://huggingface.co/settings/tokens.\")\n"
      ],
      "metadata": {
        "id": "SGSez48Y_n2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_magnus_pgn(pgn_file_path):\n",
        "    positions = []\n",
        "    with open(pgn_file_path, 'r', encoding='utf-8') as pgn_file:\n",
        "        while True:\n",
        "            game = chess.pgn.read_game(pgn_file)\n",
        "            if game is None:\n",
        "                break\n",
        "            magnus_color = \"white\" if \"Carlsen\" in game.headers.get(\"White\", \"\") else \"black\"\n",
        "            if \"Carlsen\" not in game.headers.get(\"White\", \"\") and \"Carlsen\" not in game.headers.get(\"Black\", \"\"):\n",
        "                continue\n",
        "            if \"blitz\" in game.headers.get(\"Event\", \"\").lower() or \"bullet\" in game.headers.get(\"Event\", \"\").lower():\n",
        "                continue\n",
        "            board = game.board()\n",
        "            node = game\n",
        "            while node.variations:\n",
        "                next_node = node.variation(0)\n",
        "                if (board.turn == chess.WHITE and magnus_color == \"white\") or \\\n",
        "                   (board.turn == chess.BLACK and magnus_color == \"black\"):\n",
        "                    positions.append((board.fen(), next_node.move.uci()))\n",
        "                board.push(next_node.move)\n",
        "                node = next_node\n",
        "    return positions\n",
        "\n",
        "\n",
        "def tokenize_fen(fen, tokenizer):\n",
        "    tokens = tokenizer(fen, padding=\"max_length\", max_length=64, truncation=True, return_tensors=\"pt\")\n",
        "    return tokens\n",
        "\n",
        "def encode_move(move_str):\n",
        "    move = chess.Move.from_uci(move_str)\n",
        "    return move.from_square * 64 + move.to_square\n",
        "\n",
        "\n",
        "class ChessDataset(Dataset):\n",
        "    def __init__(self, fens, moves, tokenizer):\n",
        "        self.fens = fens\n",
        "        self.moves = [encode_move(move) for move in moves]\n",
        "        self.tokenizer = tokenizer\n",
        "        self.tokenized_data = [tokenize_fen(fen, tokenizer) for fen in fens]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.fens)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tokens = self.tokenized_data[idx]\n",
        "        return {\n",
        "            \"input_ids\": tokens[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": tokens[\"attention_mask\"].squeeze(0),\n",
        "            \"labels\": torch.tensor(self.moves[idx], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "\n",
        "class ChessMoveClassifier(nn.Module):\n",
        "    def __init__(self, model_name):\n",
        "        super().__init__()\n",
        "        self.base_model = AutoModel.from_pretrained(model_name)\n",
        "        self.classifier = nn.Linear(self.base_model.config.hidden_size, 4096)  # 64x64\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids=None, labels=None):\n",
        "      outputs = self.base_model(\n",
        "          input_ids=input_ids,\n",
        "          attention_mask=attention_mask,\n",
        "          token_type_ids=token_type_ids  # pass it to the base model anyway\n",
        "      )\n",
        "      hidden_state = outputs.last_hidden_state[:, 0, :]  # use [CLS] token\n",
        "      logits = self.classifier(hidden_state)\n",
        "\n",
        "      if labels is not None:\n",
        "          loss = F.cross_entropy(logits, labels)\n",
        "          return {\"loss\": loss, \"logits\": logits}\n",
        "      return {\"logits\": logits}\n"
      ],
      "metadata": {
        "id": "BB3RaWGr_zJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    model_name = \"austindavis/ChessGPT_d12\"\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        model = ChessMoveClassifier(model_name)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        return\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    train_positions = parse_magnus_pgn(\"/content/carlsen-train.pgn\")\n",
        "    val_positions = parse_magnus_pgn(\"/content/carlsen-val.pgn\")\n",
        "    print(f\"Train positions: {len(train_positions)} | Val positions: {len(val_positions)}\")\n",
        "\n",
        "    model.base_model.gradient_checkpointing_enable()\n",
        "\n",
        "    train_fens, train_moves = zip(*train_positions)\n",
        "    val_fens, val_moves = zip(*val_positions)\n",
        "\n",
        "    train_dataset = ChessDataset(train_fens, train_moves, tokenizer)\n",
        "    # train_dataset = torch.utils.data.Subset(train_dataset, range(100))  # Using a smaller subset for faster test runs\n",
        "    val_dataset = ChessDataset(val_fens, val_moves, tokenizer)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./results',\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=32,\n",
        "        warmup_steps=100,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir='./logs',\n",
        "        logging_steps=10,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        fp16=torch.cuda.is_available(),\n",
        "        report_to=\"wandb\"\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset\n",
        "    )\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "    trainer.train()\n",
        "\n",
        "\n",
        "\n",
        "    import os\n",
        "    os.makedirs(\"/content/fine_tuned_chessgpt2\", exist_ok=True)\n",
        "    torch.save(model.state_dict(), \"/content/fine_tuned_chessgpt2/model.pt\")\n",
        "    tokenizer.save_pretrained(\"/content/fine_tuned_chessgpt2\")\n",
        "    print(\"Model saved to /content/fine_tuned_chessgpt2\")\n",
        "\n",
        "    print(\"Evaluating on validation set...\")\n",
        "    predictions = trainer.predict(val_dataset)\n",
        "\n",
        "    logits = predictions.predictions\n",
        "    true_labels = predictions.label_ids\n",
        "    predicted_indices = np.argmax(logits, axis=1)\n",
        "    correct = (predicted_indices == true_labels).sum()\n",
        "    total = len(true_labels)\n",
        "    accuracy = correct / total\n",
        "\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    def predict_magnus_move(fen):\n",
        "        model.eval()\n",
        "        inputs = tokenize_fen(fen, tokenizer)\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            predicted_index = outputs[\"logits\"].argmax(dim=-1).item()\n",
        "        from_sq = predicted_index // 64\n",
        "        to_sq = predicted_index % 64\n",
        "        move = chess.Move(from_sq, to_sq)\n",
        "        board = chess.Board(fen)\n",
        "        if move in board.legal_moves:\n",
        "            return move.uci()\n",
        "        legal_moves = list(board.legal_moves)\n",
        "        return legal_moves[0].uci() if legal_moves else \"No legal move\"\n",
        "\n",
        "    fen = \"rnbqkbnr/pppppppp/8/8/3P4/8/PPP1PPPP/RNBQKBNR b KQkq - 0 1\"\n",
        "    predicted_move = predict_magnus_move(fen)\n",
        "    print(f\"Magnus would play: {predicted_move}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "id": "jwEvKxcM_3Y4",
        "outputId": "378c6ec4-1074-4276-d98b-5febc8ec60a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Train positions: 218096 | Val positions: 28203\n",
            "Starting training...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='33074' max='40893' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [33074/40893 37:14 < 08:48, 14.80 it/s, Epoch 2.43/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>6.874200</td>\n",
              "      <td>6.749309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>6.780300</td>\n",
              "      <td>6.739650</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='40893' max='40893' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [40893/40893 46:10, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>6.874200</td>\n",
              "      <td>6.749309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>6.780300</td>\n",
              "      <td>6.739650</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>6.901300</td>\n",
              "      <td>6.736517</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to /content/fine_tuned_chessgpt2\n",
            "Evaluating on validation set...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.0107\n",
            "Magnus would play: g8h6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WV0lMKEZRsvR",
        "outputId": "733fc315-09e1-4c64-d80e-5fa35a7e2895"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Self-play"
      ],
      "metadata": {
        "id": "bmjmz61LlW0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import chess\n",
        "import chess.pgn\n",
        "import datetime\n",
        "import random\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoConfig, GPT2Model\n",
        "import torch.nn as nn\n",
        "\n",
        "# === Load model manually ===\n",
        "\n",
        "class ChessMoveClassifier(nn.Module):\n",
        "    def __init__(self, model_name, num_labels=4096):\n",
        "        super().__init__()\n",
        "        self.base_model = GPT2Model.from_pretrained(model_name)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifier = nn.Linear(self.base_model.config.n_embd, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, **kwargs):  # <-- added **kwargs\n",
        "      outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "      hidden_state = outputs.last_hidden_state[:, -1, :]  # Use last token\n",
        "      logits = self.classifier(self.dropout(hidden_state))\n",
        "      return {\"logits\": logits}\n",
        "\n",
        "\n",
        "model_dir = \"/content/fine_tuned_chessgpt2\"\n",
        "base_model = \"austindavis/ChessGPT_d12\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "model = ChessMoveClassifier(base_model)\n",
        "model.load_state_dict(torch.load(f\"{model_dir}/model.pt\", map_location=\"cpu\"))\n",
        "model.eval()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# === Tokenize FEN ===\n",
        "def tokenize_fen(fen, tokenizer):\n",
        "    return tokenizer(fen, return_tensors=\"pt\")\n",
        "\n",
        "# === Start game ===\n",
        "board = chess.Board()\n",
        "for _ in range(random.randint(4, 12)):\n",
        "    legal_moves = list(board.legal_moves)\n",
        "    if not legal_moves:\n",
        "        break\n",
        "    board.push(random.choice(legal_moves))\n",
        "\n",
        "starting_fen = board.fen()\n",
        "print(f\"Starting self-play game from position:\\n{starting_fen}\\n\")\n",
        "\n",
        "# === PGN setup ===\n",
        "game = chess.pgn.Game()\n",
        "game.headers[\"Event\"] = \"ChessGPT Self-Play (Random Start)\"\n",
        "game.headers[\"Date\"] = datetime.datetime.now().strftime(\"%Y.%m.%d\")\n",
        "game.setup(board)\n",
        "node = game\n",
        "\n",
        "# === Self-play ===\n",
        "turn = 0\n",
        "max_turns = 100\n",
        "\n",
        "while turn < max_turns and not board.is_game_over():\n",
        "    inputs = tokenize_fen(board.fen(), tokenizer)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs)[\"logits\"]\n",
        "        sorted_indices = torch.argsort(logits, dim=-1, descending=True)[0]\n",
        "\n",
        "        for idx in sorted_indices:\n",
        "            from_sq = (idx // 64).item()\n",
        "            to_sq = (idx % 64).item()\n",
        "            move = chess.Move(from_sq, to_sq)\n",
        "            if move in board.legal_moves:\n",
        "                board.push(move)\n",
        "                node = node.add_variation(move)\n",
        "                print(f\"{'White' if turn % 2 == 0 else 'Black'} plays: {move.uci()}\")\n",
        "                break\n",
        "        else:\n",
        "            print(\"[!] No valid moves predicted\")\n",
        "            break\n",
        "\n",
        "    turn += 1\n",
        "\n",
        "# === Save game ===\n",
        "game.headers[\"Result\"] = board.result()\n",
        "with open(\"self_play_game_random.pgn\", \"w\") as f:\n",
        "    f.write(str(game))\n",
        "\n",
        "print(\"Game saved to self_play_game_random.pgn\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EY2JV9YwBYK4",
        "outputId": "16063627-02e2-4a04-ce0d-b8441b79b8ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting self-play game from position:\n",
            "r1bqkbnr/p1pppppp/1pn5/8/2P1P3/5N2/PP1P1PPP/RNBQKB1R b KQkq - 1 3\n",
            "\n",
            "White plays: g8f6\n",
            "Black plays: d2d4\n",
            "White plays: e7e6\n",
            "Black plays: b1c3\n",
            "White plays: g7g6\n",
            "Black plays: g2g3\n",
            "White plays: d7d5\n",
            "Black plays: h2h3\n",
            "White plays: a7a6\n",
            "Black plays: a2a4\n",
            "White plays: h7h6\n",
            "Black plays: b2b3\n",
            "White plays: f8e7\n",
            "Black plays: c4d5\n",
            "White plays: e8g8\n",
            "Black plays: f3e5\n",
            "White plays: f8e8\n",
            "Black plays: f2f3\n",
            "White plays: c8b7\n",
            "Black plays: f1g2\n",
            "White plays: a8c8\n",
            "Black plays: e1g1\n",
            "White plays: f6e4\n",
            "Black plays: f1e1\n",
            "White plays: f7f6\n",
            "Black plays: c1e3\n",
            "White plays: e6d5\n",
            "Black plays: d1d2\n",
            "White plays: g8g7\n",
            "Black plays: d2d3\n",
            "White plays: e8g8\n",
            "Black plays: a4a5\n",
            "White plays: d8d7\n",
            "Black plays: a1c1\n",
            "White plays: d7d6\n",
            "Black plays: g3g4\n",
            "White plays: g6g5\n",
            "Black plays: c3d5\n",
            "White plays: d6d5\n",
            "Black plays: f3f4\n",
            "White plays: d5c4\n",
            "Black plays: d4d5\n",
            "White plays: c4d5\n",
            "Black plays: e3d4\n",
            "White plays: d5c4\n",
            "Black plays: g2f3\n",
            "White plays: c4d5\n",
            "Black plays: g1g2\n",
            "White plays: d5c4\n",
            "Black plays: e1g1\n",
            "White plays: c4d5\n",
            "Black plays: d4c5\n",
            "White plays: d5c4\n",
            "Black plays: c5d4\n",
            "White plays: c4d5\n",
            "Black plays: d4c5\n",
            "White plays: d5c4\n",
            "Black plays: c5d4\n",
            "White plays: c4d5\n",
            "Black plays: d4c5\n",
            "White plays: d5c4\n",
            "Black plays: c5d4\n",
            "White plays: c4d5\n",
            "Black plays: d4c5\n",
            "White plays: d5c4\n",
            "Black plays: c5d4\n",
            "Game saved to self_play_game_random.pgn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Base model VS magnus"
      ],
      "metadata": {
        "id": "UyGvgmyPnP2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import chess\n",
        "import chess.pgn\n",
        "import datetime\n",
        "import torch\n",
        "import random\n",
        "from transformers import AutoTokenizer, GPT2Model\n",
        "import torch.nn as nn\n",
        "\n",
        "# === Model class ===\n",
        "class ChessMoveClassifier(nn.Module):\n",
        "    def __init__(self, model_name, num_labels=4096):\n",
        "        super().__init__()\n",
        "        self.base_model = GPT2Model.from_pretrained(model_name)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifier = nn.Linear(self.base_model.config.n_embd, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, **kwargs):\n",
        "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        hidden_state = outputs.last_hidden_state[:, -1, :]\n",
        "        logits = self.classifier(self.dropout(hidden_state))\n",
        "        return {\"logits\": logits}\n",
        "\n",
        "# === Setup device ===\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# === Load tokenizer ===\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"austindavis/ChessGPT_d12\")\n",
        "\n",
        "# === Load fine-tuned MagnusGPT ===\n",
        "magnus_model = ChessMoveClassifier(\"austindavis/ChessGPT_d12\")\n",
        "magnus_model.load_state_dict(torch.load(\"/content/fine_tuned_chessgpt2/model.pt\", map_location=device))\n",
        "magnus_model.to(device)\n",
        "magnus_model.eval()\n",
        "\n",
        "# === Load base model ===\n",
        "base_model = ChessMoveClassifier(\"austindavis/ChessGPT_d12\")\n",
        "base_model.to(device)\n",
        "base_model.eval()\n",
        "\n",
        "# === Tokenizer wrapper ===\n",
        "def tokenize_fen(fen):\n",
        "    return tokenizer(fen, return_tensors=\"pt\")\n",
        "\n",
        "# === Start from random legal position ===\n",
        "board = chess.Board()\n",
        "for _ in range(random.randint(4, 12)):\n",
        "    legal_moves = list(board.legal_moves)\n",
        "    if not legal_moves:\n",
        "        break\n",
        "    board.push(random.choice(legal_moves))\n",
        "\n",
        "starting_fen = board.fen()\n",
        "print(f\"Starting from position:\\n{starting_fen}\\n\")\n",
        "\n",
        "# === Setup PGN ===\n",
        "game = chess.pgn.Game()\n",
        "game.headers[\"Event\"] = \"MagnusGPT vs BaseGPT\"\n",
        "game.headers[\"Date\"] = datetime.datetime.now().strftime(\"%Y.%m.%d\")\n",
        "game.setup(board)\n",
        "node = game\n",
        "\n",
        "# === Self-play match ===\n",
        "turn = 0\n",
        "max_turns = 100\n",
        "\n",
        "while turn < max_turns and not board.is_game_over():\n",
        "    inputs = tokenize_fen(board.fen())\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # White = MagnusGPT, Black = BaseGPT\n",
        "    model = magnus_model if board.turn == chess.WHITE else base_model\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs)[\"logits\"]\n",
        "        sorted_indices = torch.argsort(logits, dim=-1, descending=True)[0]\n",
        "\n",
        "        for idx in sorted_indices:\n",
        "            from_sq = (idx // 64).item()\n",
        "            to_sq = (idx % 64).item()\n",
        "            move = chess.Move(from_sq, to_sq)\n",
        "            if move in board.legal_moves:\n",
        "                board.push(move)\n",
        "                node = node.add_variation(move)\n",
        "                print(f\"{'White (Magnus)' if board.turn == chess.BLACK else 'Black (Base)'} plays: {move.uci()}\")\n",
        "                break\n",
        "        else:\n",
        "            print(\"[!] No valid moves predicted\")\n",
        "            break\n",
        "\n",
        "    turn += 1\n",
        "\n",
        "# === Finalize game ===\n",
        "game.headers[\"Result\"] = board.result()\n",
        "with open(\"magnus_vs_base.pgn\", \"w\") as f:\n",
        "    f.write(str(game))\n",
        "\n",
        "print(\"Game saved to magnus_vs_base.pgn\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CzPbNN1AN-f",
        "outputId": "6a14fce8-09a7-46e6-9993-6de9e3b726e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting from position:\n",
            "rnbqkbnr/pp1pp1pp/5p2/2p5/8/2N5/PPPPPPPP/1RBQKBNR w Kkq - 0 3\n",
            "\n",
            "White (Magnus) plays: g1f3\n",
            "Black (Base) plays: b8a6\n",
            "White (Magnus) plays: d2d4\n",
            "Black (Base) plays: a6c7\n",
            "White (Magnus) plays: e2e4\n",
            "Black (Base) plays: h7h5\n",
            "White (Magnus) plays: g2g3\n",
            "Black (Base) plays: a8b8\n",
            "White (Magnus) plays: h2h3\n",
            "Black (Base) plays: c5d4\n",
            "White (Magnus) plays: a2a4\n",
            "Black (Base) plays: e8f7\n",
            "White (Magnus) plays: b2b3\n",
            "Black (Base) plays: c7b5\n",
            "White (Magnus) plays: f3e5\n",
            "Black (Base) plays: f7e6\n",
            "White (Magnus) plays: f2f3\n",
            "Black (Base) plays: b5d6\n",
            "White (Magnus) plays: f1g2\n",
            "Black (Base) plays: d6e8\n",
            "White (Magnus) plays: e1g1\n",
            "Black (Base) plays: e8c7\n",
            "White (Magnus) plays: f1e1\n",
            "Black (Base) plays: g8h6\n",
            "White (Magnus) plays: c1e3\n",
            "Black (Base) plays: h6g8\n",
            "White (Magnus) plays: d1d2\n",
            "Black (Base) plays: g8h6\n",
            "White (Magnus) plays: d2d4\n",
            "Black (Base) plays: h6g8\n",
            "White (Magnus) plays: a4a5\n",
            "Black (Base) plays: g8h6\n",
            "White (Magnus) plays: d4d5\n",
            "Black (Base) plays: c7d5\n",
            "White (Magnus) plays: e4d5\n",
            "Black (Base) plays: e6f5\n",
            "White (Magnus) plays: g3g4\n",
            "Black (Base) plays: f5e5\n",
            "White (Magnus) plays: e3d4\n",
            "Black (Base) plays: e5d6\n",
            "White (Magnus) plays: d4e5\n",
            "Black (Base) plays: f6e5\n",
            "White (Magnus) plays: f3f4\n",
            "Black (Base) plays: h6g8\n",
            "White (Magnus) plays: g4g5\n",
            "Black (Base) plays: g8f6\n",
            "White (Magnus) plays: g2f3\n",
            "Black (Base) plays: e5e4\n",
            "White (Magnus) plays: g1g2\n",
            "Black (Base) plays: e4e3\n",
            "White (Magnus) plays: e1g1\n",
            "Black (Base) plays: h5h4\n",
            "White (Magnus) plays: f4f5\n",
            "Black (Base) plays: h8h5\n",
            "White (Magnus) plays: g5f6\n",
            "Black (Base) plays: h5h6\n",
            "White (Magnus) plays: c3e4\n",
            "Black (Base) plays: d6d5\n",
            "White (Magnus) plays: c2c4\n",
            "Black (Base) plays: d5c6\n",
            "White (Magnus) plays: c4c5\n",
            "Black (Base) plays: g7f6\n",
            "White (Magnus) plays: b3b4\n",
            "Black (Base) plays: c6d5\n",
            "White (Magnus) plays: b4b5\n",
            "Black (Base) plays: d8b6\n",
            "White (Magnus) plays: g1f1\n",
            "Black (Base) plays: b6b5\n",
            "White (Magnus) plays: f1e1\n",
            "Black (Base) plays: b5d3\n",
            "White (Magnus) plays: e1g1\n",
            "Black (Base) plays: d3d4\n",
            "White (Magnus) plays: g1f1\n",
            "Black (Base) plays: d4e4\n",
            "White (Magnus) plays: f1e1\n",
            "Black (Base) plays: d5c5\n",
            "White (Magnus) plays: e1g1\n",
            "Black (Base) plays: e4e5\n",
            "White (Magnus) plays: g1f1\n",
            "Black (Base) plays: e5e4\n",
            "White (Magnus) plays: f1e1\n",
            "Black (Base) plays: e4e5\n",
            "White (Magnus) plays: e1g1\n",
            "Black (Base) plays: e5e4\n",
            "White (Magnus) plays: g1f1\n",
            "Black (Base) plays: e4e5\n",
            "White (Magnus) plays: f1e1\n",
            "Black (Base) plays: e5h2\n",
            "White (Magnus) plays: g2f1\n",
            "Black (Base) plays: h2g3\n",
            "White (Magnus) plays: f1e2\n",
            "Black (Base) plays: g3d6\n",
            "White (Magnus) plays: e1g1\n",
            "Black (Base) plays: d6e6\n",
            "White (Magnus) plays: g1g2\n",
            "Black (Base) plays: c5d4\n",
            "White (Magnus) plays: g2g3\n",
            "Black (Base) plays: d4c5\n",
            "White (Magnus) plays: g3g4\n",
            "Black (Base) plays: e6f7\n",
            "Game saved to magnus_vs_base.pgn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Does the model play like magnus?"
      ],
      "metadata": {
        "id": "_dGp0E1rn-Cs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import chess.pgn\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import GPT2Model\n",
        "import torch.nn as nn\n",
        "import io\n",
        "\n",
        "# === Model class ===\n",
        "class ChessMoveClassifier(nn.Module):\n",
        "    def __init__(self, model_name, num_labels=4096):\n",
        "        super().__init__()\n",
        "        self.base_model = GPT2Model.from_pretrained(model_name)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifier = nn.Linear(self.base_model.config.n_embd, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, **kwargs):\n",
        "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        hidden_state = outputs.last_hidden_state[:, -1, :]\n",
        "        logits = self.classifier(self.dropout(hidden_state))\n",
        "        return {\"logits\": logits}\n",
        "\n",
        "# === Tokenizer wrapper ===\n",
        "def tokenize_fen(fen, tokenizer):\n",
        "    return tokenizer(fen, return_tensors=\"pt\")\n",
        "\n",
        "# === Load model and tokenizer ===\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_name = \"austindavis/ChessGPT_d12\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/fine_tuned_chessgpt2\")\n",
        "model = ChessMoveClassifier(model_name)\n",
        "model.load_state_dict(torch.load(\"/content/fine_tuned_chessgpt2/model.pt\", map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# === PGN of real Magnus game (as Black) ===\n",
        "pgn_text = \"\"\"[Event \"5th OIBM\"]\n",
        "[Site \"Bad Wiessee GER\"]\n",
        "[Date \"2001.10.27\"]\n",
        "[Round \"1\"]\n",
        "[White \"Kacheishvili,G\"]\n",
        "[Black \"Carlsen,M\"]\n",
        "[Result \"1-0\"]\n",
        "[WhiteElo \"2583\"]\n",
        "[BlackElo \"2072\"]\n",
        "[ECO \"E32\"]\n",
        "\n",
        "1.d4 Nf6 2.c4 e6 3.Nc3 Bb4 4.Qc2 O-O 5.a3 Bxc3+ 6.Qxc3 b6 7.Bg5 Bb7 8.e3 c5\n",
        "9.dxc5 bxc5 10.Ne2 Nc6 11.Ng3 Qa5 12.Bxf6 gxf6 13.Qxa5 Nxa5 14.Nh5 Rfd8 15.O-O-O Kf8\n",
        "16.Nxf6 Ke7 17.Nh5 Ba6 18.Nf4 Bxc4 19.Bxc4 Nxc4 20.Rhe1 Rab8 21.Re2 d5 22.Rc2 Na5\n",
        "23.Kb1 c4 24.Ne2 Nb7 25.e4 Nc5 26.exd5 Rxd5 27.Nc3 Rg5 28.f4 Rh5 29.Rd4 Rxh2\n",
        "30.Rxc4 Nd3 31.Ka2 Rb7 32.Ne4 Rd7 33.Nc5 Nxc5 34.Rxc5 Kf6 35.Ra5 Rb7 36.f5 e5\n",
        "37.Re2 Re7 38.Ra6+ Kg5 39.f6 Rb7 40.Rxe5+ Kg6 41.Re7 Rb8 42.Raxa7 Rxg2 43.Rab7 Rxb7\n",
        "44.Rxb7 h5 45.a4 Rg4 46.Ka3 h4 47.Rb8 Rg3+ 48.b3 Kxf6 49.Rh8 Kg5 50.a5 Re3\n",
        "51.Ka4 Re6 52.b4 f5 53.b5 Re1 54.a6 Ra1+ 55.Kb4 f4 56.Kc5 f3 57.b6 Ra5+ 58.Kd4 Rxa6\n",
        "59.b7 Rb6 60.b8=Q Rxb8 61.Rxb8 Kf4 62.Rf8+ Kg3 63.Ke3 1-0\n",
        "\"\"\"\n",
        "\n",
        "# === Parse game ===\n",
        "game = chess.pgn.read_game(io.StringIO(pgn_text))\n",
        "board = game.board()\n",
        "magnus_moves = []\n",
        "fens = []\n",
        "\n",
        "# Magnus is Black, so collect FENs before each Black move\n",
        "for idx, move in enumerate(game.mainline_moves()):\n",
        "    if idx % 2 == 1:  # Black's turn\n",
        "        fens.append(board.fen())\n",
        "        magnus_moves.append(move.uci())\n",
        "    board.push(move)\n",
        "\n",
        "# === Run predictions and compare ===\n",
        "correct = 0\n",
        "total = len(fens)\n",
        "\n",
        "for i in range(total):\n",
        "    fen = fens[i]\n",
        "    true_move = magnus_moves[i]\n",
        "    inputs = tokenize_fen(fen, tokenizer)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs)[\"logits\"]\n",
        "        sorted_indices = torch.argsort(logits, dim=-1, descending=True)[0]\n",
        "\n",
        "        predicted_move = None\n",
        "        board = chess.Board(fen)\n",
        "        for idx in sorted_indices:\n",
        "            from_sq = (idx // 64).item()\n",
        "            to_sq = (idx % 64).item()\n",
        "            move = chess.Move(from_sq, to_sq)\n",
        "            if move in board.legal_moves:\n",
        "                predicted_move = move.uci()\n",
        "                break\n",
        "\n",
        "        is_correct = (predicted_move == true_move)\n",
        "        correct += int(is_correct)\n",
        "\n",
        "        print(f\"[{i+1}] True: {true_move} | Predicted: {predicted_move} | {'✅' if is_correct else '❌'}\")\n",
        "\n",
        "# === Accuracy report ===\n",
        "accuracy = correct / total\n",
        "print(f\"\\nMagnus Match Accuracy: {accuracy:.2%} ({correct}/{total})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdFG_T8DoAz9",
        "outputId": "c4024d19-7a80-46e5-dc14-39c6add3566c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] True: g8f6 | Predicted: g8f6 | ✅\n",
            "[2] True: e7e6 | Predicted: b8c6 | ❌\n",
            "[3] True: f8b4 | Predicted: b8c6 | ❌\n",
            "[4] True: e8g8 | Predicted: e8g8 | ✅\n",
            "[5] True: b4c3 | Predicted: b8c6 | ❌\n",
            "[6] True: b7b6 | Predicted: b8c6 | ❌\n",
            "[7] True: c8b7 | Predicted: b8c6 | ❌\n",
            "[8] True: c7c5 | Predicted: b8c6 | ❌\n",
            "[9] True: b6c5 | Predicted: b8c6 | ❌\n",
            "[10] True: b8c6 | Predicted: b8c6 | ✅\n",
            "[11] True: d8a5 | Predicted: g7g6 | ❌\n",
            "[12] True: g7f6 | Predicted: g7g6 | ❌\n",
            "[13] True: c6a5 | Predicted: d7d5 | ❌\n",
            "[14] True: f8d8 | Predicted: d7d5 | ❌\n",
            "[15] True: g8f8 | Predicted: d7d5 | ❌\n",
            "[16] True: f8e7 | Predicted: d7d5 | ❌\n",
            "[17] True: b7a6 | Predicted: d7d5 | ❌\n",
            "[18] True: a6c4 | Predicted: d7d5 | ❌\n",
            "[19] True: a5c4 | Predicted: d7d5 | ❌\n",
            "[20] True: a8b8 | Predicted: d7d5 | ❌\n",
            "[21] True: d7d5 | Predicted: d7d5 | ✅\n",
            "[22] True: c4a5 | Predicted: a7a6 | ❌\n",
            "[23] True: c5c4 | Predicted: a7a6 | ❌\n",
            "[24] True: a5b7 | Predicted: a7a6 | ❌\n",
            "[25] True: b7c5 | Predicted: a7a6 | ❌\n",
            "[26] True: d8d5 | Predicted: a7a6 | ❌\n",
            "[27] True: d5g5 | Predicted: a7a6 | ❌\n",
            "[28] True: g5h5 | Predicted: a7a6 | ❌\n",
            "[29] True: h5h2 | Predicted: a7a6 | ❌\n",
            "[30] True: c5d3 | Predicted: a7a6 | ❌\n",
            "[31] True: b8b7 | Predicted: a7a6 | ❌\n",
            "[32] True: b7d7 | Predicted: a7a6 | ❌\n",
            "[33] True: d3c5 | Predicted: d7d5 | ❌\n",
            "[34] True: e7f6 | Predicted: d7d5 | ❌\n",
            "[35] True: d7b7 | Predicted: d7d5 | ❌\n",
            "[36] True: e6e5 | Predicted: a7a6 | ❌\n",
            "[37] True: b7e7 | Predicted: a7a6 | ❌\n",
            "[38] True: f6g5 | Predicted: e7e6 | ❌\n",
            "[39] True: e7b7 | Predicted: e7e6 | ❌\n",
            "[40] True: g5g6 | Predicted: g5g4 | ❌\n",
            "[41] True: b7b8 | Predicted: h7h6 | ❌\n",
            "[42] True: h2g2 | Predicted: h7h6 | ❌\n",
            "[43] True: b8b7 | Predicted: g2g3 | ❌\n",
            "[44] True: h7h5 | Predicted: g2g3 | ❌\n",
            "[45] True: g2g4 | Predicted: g2g3 | ❌\n",
            "[46] True: h5h4 | Predicted: g6g5 | ❌\n",
            "[47] True: g4g3 | Predicted: g6g5 | ❌\n",
            "[48] True: g6f6 | Predicted: g3g4 | ❌\n",
            "[49] True: f6g5 | Predicted: g3g4 | ❌\n",
            "[50] True: g3e3 | Predicted: f7f6 | ❌\n",
            "[51] True: e3e6 | Predicted: f7f6 | ❌\n",
            "[52] True: f7f5 | Predicted: f7f6 | ❌\n",
            "[53] True: e6e1 | Predicted: e6e5 | ❌\n",
            "[54] True: e1a1 | Predicted: e1g1 | ❌\n",
            "[55] True: f5f4 | Predicted: a1c1 | ❌\n",
            "[56] True: f4f3 | Predicted: a1c1 | ❌\n",
            "[57] True: a1a5 | Predicted: a1c1 | ❌\n",
            "[58] True: a5a6 | Predicted: a5a4 | ❌\n",
            "[59] True: a6b6 | Predicted: a6a5 | ❌\n",
            "[60] True: b6b8 | Predicted: b6b5 | ❌\n",
            "[61] True: g5f4 | Predicted: g5f6 | ❌\n",
            "[62] True: f4g3 | Predicted: f4g5 | ❌\n",
            "\n",
            "Magnus Match Accuracy: 6.45% (4/62)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accuracy test training data with carlsons games and the model"
      ],
      "metadata": {
        "id": "Mpprw9vDoaNz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import chess.pgn\n",
        "import torch\n",
        "from transformers import AutoTokenizer, GPT2Model\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "\n",
        "# === Model class ===\n",
        "class ChessMoveClassifier(nn.Module):\n",
        "    def __init__(self, model_name, num_labels=4096):\n",
        "        super().__init__()\n",
        "        self.base_model = GPT2Model.from_pretrained(model_name)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifier = nn.Linear(self.base_model.config.n_embd, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, **kwargs):\n",
        "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        hidden_state = outputs.last_hidden_state[:, -1, :]\n",
        "        logits = self.classifier(self.dropout(hidden_state))\n",
        "        return {\"logits\": logits}\n",
        "\n",
        "# === Tokenizer wrapper ===\n",
        "def tokenize_fen(fen, tokenizer):\n",
        "    return tokenizer(fen, return_tensors=\"pt\")\n",
        "\n",
        "# Load tokenizer and model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_name = \"austindavis/ChessGPT_d12\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/fine_tuned_chessgpt2\")\n",
        "model = ChessMoveClassifier(model_name)\n",
        "model.load_state_dict(torch.load(\"/content/fine_tuned_chessgpt2/model.pt\", map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# First pass to count games\n",
        "with open(\"/content/carlsen-train.pgn\", \"r\", encoding=\"utf-8\") as f:\n",
        "    game_count = sum(1 for line in f if line.strip() == \"\")\n",
        "\n",
        "# Evaluation loop with live accuracy updates\n",
        "pgn_path = \"/content/carlsen-train.pgn\"\n",
        "total = 0\n",
        "correct = 0\n",
        "\n",
        "with open(pgn_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    with tqdm(total=game_count, desc=\"Processing games\") as pbar:\n",
        "        while True:\n",
        "            game = chess.pgn.read_game(f)\n",
        "            if game is None:\n",
        "                break\n",
        "\n",
        "            if \"Carlsen\" not in game.headers.get(\"White\", \"\") and \"Carlsen\" not in game.headers.get(\"Black\", \"\"):\n",
        "                pbar.update(1)\n",
        "                continue\n",
        "\n",
        "            magnus_color = \"white\" if \"Carlsen\" in game.headers.get(\"White\", \"\") else \"black\"\n",
        "            board = game.board()\n",
        "\n",
        "            for idx, move in enumerate(game.mainline_moves()):\n",
        "                if (magnus_color == \"white\" and idx % 2 == 0) or (magnus_color == \"black\" and idx % 2 == 1):\n",
        "                    fen = board.fen()\n",
        "                    true_move = move.uci()\n",
        "\n",
        "                    inputs = tokenize_fen(fen, tokenizer)\n",
        "                    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "                    with torch.no_grad():\n",
        "                        logits = model(**inputs)[\"logits\"]\n",
        "                        sorted_indices = torch.argsort(logits, dim=-1, descending=True)[0]\n",
        "\n",
        "                        predicted_move = None\n",
        "                        temp_board = chess.Board(fen)\n",
        "                        for idxx in sorted_indices:\n",
        "                            from_sq = (idxx // 64).item()\n",
        "                            to_sq = (idxx % 64).item()\n",
        "                            candidate_move = chess.Move(from_sq, to_sq)\n",
        "                            if candidate_move in temp_board.legal_moves:\n",
        "                                predicted_move = candidate_move.uci()\n",
        "                                break\n",
        "\n",
        "                    total += 1\n",
        "                    if predicted_move == true_move:\n",
        "                        correct += 1\n",
        "\n",
        "                    # Update progress bar description with live accuracy\n",
        "                    if total % 10 == 0:\n",
        "                        acc = correct / total\n",
        "                        pbar.set_postfix_str(f\"Acc: {acc:.4f} ({correct}/{total})\")\n",
        "\n",
        "                board.push(move)\n",
        "\n",
        "            pbar.update(1)\n",
        "\n",
        "# Final output\n",
        "accuracy = correct / total if total > 0 else 0\n",
        "print(f\"\\nFinal Accuracy: {accuracy:.4f} ({correct}/{total})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "AmBJot3rofFz",
        "outputId": "211dffdd-3463-4c7e-a22a-7188c3640b63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating:   0%|          | 0/11906 [00:33<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/lib/python3.11/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m                 \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    857\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-74-839519430.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-74-839519430.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_games\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Evaluating\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimap_unordered\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m                     \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                     \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    859\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 861\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    862\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m                     \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi, upload_folder\n",
        "import os\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "repo_id = \"strumber/magnusTransformer\"\n",
        "local_folder = \"./fine_tuned_chessgpt2\"\n",
        "\n",
        "api = HfApi()\n",
        "api.create_repo(repo_id=repo_id, token=hf_token, exist_ok=True)\n",
        "\n",
        "upload_folder(\n",
        "    folder_path=local_folder,\n",
        "    repo_id=repo_id,\n",
        "    token=hf_token,\n",
        "    repo_type=\"model\"\n",
        ")\n",
        "\n",
        "print(\"Model & tokenizer pushed to Hugging Face Hub!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "c993b20a6b584599b1743f987704fd63",
            "569f8300629a4455b8aa054c8b6f8ace",
            "7a8f4100d68c458b8cf9d1cefbef25dc",
            "d725a2afc2ec47799594d3a360552b8f",
            "d4c4d914658b43da8d6a324c3db4a7dc",
            "699c78c1c8b54a4bb831d70ba0d24d8f",
            "3d8aae51801f42b7ac622bdb8836a857",
            "4b982d23d74740ea9fff91e4fd633944",
            "0537f75172a5472a9ba1cd3c487bf24b",
            "f2b6619a6a9c43cc94f914c1a40f75a7",
            "0631bf2b5ce243329c8e11567442f23f"
          ]
        },
        "id": "mGr7CGMVrgQs",
        "outputId": "24c4b839-42c7-4229-fdbd-2d22c68e1908"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.pt:   0%|          | 0.00/356M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c993b20a6b584599b1743f987704fd63"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model & tokenizer pushed to Hugging Face Hub!\n"
          ]
        }
      ]
    }
  ]
}